# Tiny Vision Transformer (ViT)

My goal with this repo was to create the simplest possible implementation of a ViT. The code follows the paper ["An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. To keep things tiny, we train the model on 32x32 images from the CIFAR-10 dataset using 4x4 image patches. The code is thoroughly documented to make it as easy as possible for newcomers to ViTs to get up to speed with the basics of the model.
